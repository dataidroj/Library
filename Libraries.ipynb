{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "\n",
    "#Info\n",
    "df.describe()\n",
    "df.describe(include = \"all\")\n",
    "df.info\n",
    "df.info()\n",
    "df.head\n",
    "df.tail\n",
    "df.dtypes\n",
    "df['column']. values_counts()  \n",
    "\n",
    "\n",
    "#Import Data\n",
    "#Data Wrangling\n",
    "    #Missing Values\n",
    "    #Format Data\n",
    "    #Data Normalization\n",
    "    #categorical -numeric\n",
    "    \n",
    "df.dropna(subset=[\"price\"], axis=0)\n",
    "\n",
    "#Binning\n",
    "binwidth = int((max(df['price'])-min(df['price']))/4)\n",
    "bins = range(min(df['price']), max(df['price']), binwidth)\n",
    "bins = np.arange(min(df['price']), max(df['price']), binwidth)\n",
    "bins = np.linspace(min(df[\"horsepower\"]), max(df[\"horsepower\"]), 4)\n",
    "group_names = ['Low', 'Medium', 'High']\n",
    "df['price-binned'] = pd.cut(df['price'], bins, labels = group_names)\n",
    "\n",
    "#to numeric\n",
    "df.get_dummies\n",
    "\n",
    "#null data\n",
    "missing_data = df.isnull()\n",
    "\n",
    "\n",
    "df['num-of-doors'].value_counts()\n",
    "df['num-of-doors'].value_counts().idxmax()\n",
    "df[\"num-of-doors\"].replace(np.nan, \"four\", inplace = True)\n",
    "\n",
    "avg_5=df['peak-rpm'].astype('float').mean(axis=0)\n",
    "df['peak-rpm'].replace(np.nan, avg_5, inplace= True)\n",
    "\n",
    "# simply drop whole row with NaN in \"price\" column\n",
    "df.dropna(subset=[\"price\"], axis=0, inplace = True)\n",
    "\n",
    "# reset index, because we droped two rows\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "#Exploratory Data Analysis (descriptive stadistics, groupby, anova, correlation, correlation stadistics-- pearson, heatmap)\n",
    "    #Summarize main characteristics of the data\n",
    "    #Gain better understanding\n",
    "    #Uncover relatiopnships\n",
    "    #Extract important variables\n",
    "\n",
    "df.describe()\n",
    "# Relationship between 2 variables\n",
    "#scatter plot\n",
    "#boxplot\n",
    "\n",
    "#groupby \n",
    "df_test = df['a','b','c']\n",
    "df_group = df_test.groupby(['a', 'b'], as_index=False).mean()\n",
    "df_pivot = df_grp.pivot(index = 'a', columns = 'b')  ----- heatmap\n",
    " \n",
    "#Anova (statistical comparison of groups) --  avg comparison of groups example \n",
    "#find correlation betwween groups of categorical variable\n",
    "\n",
    "#     F-test score -- small imply poor correlation \n",
    "#     p-value\n",
    "    \n",
    "##correlation\n",
    "sns.regplot(x=\"engin_size\", y = \"prices\", data=df)\n",
    "Plt.ylim(0,)\n",
    "\n",
    "#correlation pearson\n",
    "    #correlation coefficient\n",
    "    #p-value\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Development\n",
    "# 1.Simple and Multiple Linear Regression\n",
    "from sklearn.lineal_model import LinearRegression\n",
    "lm =LinearRegression()\n",
    "lm.fit(X,Y)\n",
    "Yhat = lm.predict(X)\n",
    "# 2. Model Eavluation using visualization\n",
    "    Regression Plot\n",
    "        relation\n",
    "        strength of the correlation\n",
    "        direction of the relationship\n",
    "        #seaborn regplot  & residual plot residplot\n",
    "    distribution plot (histogram)\n",
    "    ax1 = sns.distplot(df['price'], hist=False, color = 'r', label = 'Actual Value')\n",
    "    sns.distplot(Yhat, hist=False, color = 'b', label=\"Fitted Values\", ax = ax1)\n",
    "# 3. Polynomial regression and Pipelines\n",
    "    f = np.polyfit(x,y,3)\n",
    "    p = np.polydl(f)\n",
    "    \n",
    "# 4. R-squared and MSE for In-Sample Evaluation\n",
    "    from sklearn.metrix import mean_squared_error\n",
    "    mean_squared_error(df['price'], Y_predict_simple_fit)\n",
    "    \n",
    "    R2 the percentatge of variation of the target variable(Y) that is explained by the linear model\n",
    "\n",
    "# 5. Prediction and Decision Making\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 0)\n",
    "\n",
    "cross_validation cross_val_score()\n",
    "from sklearn.model_selection import cross_val_score (cross_val_predict)\n",
    "scores = cross_val_score(lr, x_data, y_data, cv=3)\n",
    "np.mean(scores)\n",
    "cross_val_predict\n",
    "\n",
    "#Overfit, underfit, model selection\n",
    "--programa para calcular diferentes polinomios\n",
    "#Ridge Regression (for overfiting)\n",
    "from sklearn.linear_model import Ridge\n",
    "RigeModel = Ridge(alpha=0.1)\n",
    "RigeModel.fit(X,y)\n",
    "Yhat =RigeModel.predict(X)\n",
    "#Grid Search - hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
